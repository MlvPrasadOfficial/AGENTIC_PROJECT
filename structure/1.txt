# Agentic Copilot: LLM-Powered Analytics & Conversational BI Platform

## Overview

A next-gen Agentic Copilot built for business analytics using Llama 3.1, LangChain, LangGraph, Pinecone, FastAPI (Render), Next.js (Vercel). All agents are hierarchical, glassmorphic, 3D components, matching your UI/UX vision.

---

## 1. High-Level Architecture (UI & Workflow)

* **UI Layout: 2 Columns + Full-Width Visualization Panel (Next.js, Glassmorphic, 3D)**

  * **Left Column**

    * **Top:** File Upload (Hierarchical glassmorphic card)

      * Drag/drop upload, rows indexed, preview of file
      * Below: Dataframe/table preview (first 5 rows)
    * **Below Upload:** RAG Chatbot glass component

      * Chat with AI Copilot, only final answers shown here (post-debate)
  * **Right Column**

    * **Agent Pipeline (vertical, glassmorphic, hierarchical, each collapsible)**

      * File Upload Agent (for file ingestion, triggers next)
      * **Data Profile Agent:**

        * Profiles uploaded data (schema, types, nulls, stats)
        * Output appears as dropdown glass card (after file upload)
      * **Planning Agent:**

        * Receives user query, selects downstream agent
        * Routes to Insight Agent or Viz Agent
      * **Insight Agent:**

        * Handles insight/narrative queries, generates textual insights
      * **Viz Agent:**

        * Handles visualization queries, outputs D3.js chart config/data
      * **Critique Agent:**

        * Critiques response from previous agent (insight/viz), suggests improvements
      * **Debate Agent:**

        * Multi-perspective analysis, selects best/most robust final response
        * Only this response is shown in chat UI (final output to user)
      * **Report Agent:**

        * Collects all step responses, generates PDF of entire workflow (download link shown in chat panel)
    * All agents use LangChain, connected via LangGraph
    * Every agent's output available in collapsible glass card, with dropdown for full logs/responses
  * **Below Both Columns (Full Width):**

    * **D3.js Visualization Glass Panel**

      * All charts and advanced data visuals are rendered here in a wide, persistent, beautiful glassmorphic component
      * Updates in real-time as Viz Agent answers change

## 2. Core Workflow

1. **User uploads file:**

   * File Upload Agent → Data Profile Agent (profiling + preview)
2. **User asks query:**

   * Planning Agent routes to either:

     * **Insight Agent:** returns textual insights
     * **Viz Agent:** returns chart, renders D3.js below chat
3. **Critique Agent:** critiques selected agent response
4. **Debate Agent:** multi-perspective, finalizes answer (only this shown in chat)
5. **Report Agent:** captures full end-to-end workflow, user can download PDF
6. **UI:** All agent responses visible as collapsible, hierarchical glassmorphic cards (right column), chat panel only shows debate agent (final) response

---

## 3. Data Flow

* **File Upload → Data Profiling (mandatory before queries)**
* **Query → Planning → (Insight or Viz Agent)**
* **Each step is critiqued, debated, finalized before user sees result**
* **All responses are transparent/logged in right-side pipeline, final only in chat**
* **PDF report available at end**

---

## 4. Agent Descriptions (Revised)

* **File Upload Agent:** Manages file upload event
* **Data Profile Agent:** Profiles data, outputs schema, missing values, stats
* **Planning Agent:** Parses user query, routes to proper downstream agent
* **Insight Agent:** Handles insight/narrative requests, provides deep, textual insights
* **Viz Agent:** Handles visualization requests, produces D3.js-ready chart configs/data
* **Critique Agent:** Critiques output from Insight/Viz Agent
* **Debate Agent:** Multi-perspective, chooses final best answer (chat output)
* **Report Agent:** Gathers all steps, outputs downloadable PDF

---

## 5. Embedding & Retrieval

* Embedding/model configuration should be set and controlled exclusively via environment variables (`.env`) for flexibility, security, and easy deployment.
* Uses Pinecone as the only vector database (no FAISS).
* All embedding model details, API keys, model selection (OLLAMA\_MODEL), and Pinecone configs are sourced from `.env` (see Environment Configuration section).
* On file upload: Document & table chunking, embedding, and indexing at upload time.
* Retrieval for Insight/Viz queries as triggered by agent workflow.

## 6. Deployment

* **Frontend:** Vercel (Next.js 14+, Tailwind, shadcn/ui, D3.js, Lucide-react)
* **Backend:** Render.com (FastAPI + LangChain + LangGraph + Pinecone + OLLAMA)
* **Docker, GitHub Actions** for CI/CD

---

## 7. Tech Stack

* Llama 3.1, LangChain, LangGraph, Pinecone, FASTAPI
* Next.js, Tailwind, shadcn/ui, D3.js, Lucide-react
* Docker, GitHub Actions, Render, Vercel

---

## 8. Sample Use Cases

* Upload HR/Finance data, ask “Show attrition trends as chart” (Viz Agent), “Give key churn insights” (Insight Agent)
* Download full PDF of workflow & agent outputs

---

## 9. MAANG-Ready Resume Bullets

* Designed, architected and built agentic, hierarchical, glassmorphic copilot with LangChain/LangGraph
* Built multi-agent orchestration (Data Profile, Planning, Insight, Viz, Critique, Debate, Report)
* Fully transparent, stepwise agent pipeline; final result from robust debate
* End-to-end, single-page app (Vercel+Render+Pinecone)

---

## 10. UI/UX Notes

* All agent responses shown as collapsible, hierarchical glass cards in right pipeline
* D3.js visualizations for viz queries below chat
* PDF report available after every workflow
* Only final response after debate agent is in chat component

### Advanced MAANG-Level UI/UX Enhancements

1. **Animated Glassmorphism & Depth**

   * Subtle 3D drop shadows, animated glass gradients on hover/focus for each agent card.
   * Parallax background stars/particles for "AI-native" depth.
2. **Agent Pipeline Visualization**

   * Vertical stepper/timeline (right column) with animated "active" status per agent.
   * Visual connectors between agents (animated lines/glow during processing).
   * Collapse/expand agent logs with smooth animation.
3. **Microinteractions & Feedback**

   * Agent cards animate (tick/check, glow, color shift) when steps complete.
   * Skeleton loaders/shimmer effects during processing instead of spinners.
4. **User Guidance**

   * "What can I ask?" and "Try these prompts" inline suggestions below upload/chat.
   * Dynamic hints based on data profile (e.g., highlight suitable queries).
5. **Richer Data Preview**

   * Dataframe preview with inline sort/filter.
   * Auto-detect/callout issues (nulls, weird dates) directly in the table.
6. **Persistent Chat & Viz Area**

   * Chat split: messages and always-on visualization panel below.
7. **Theming & Accessibility**

   * Dark mode with user-adjustable accent color (blue/purple/green).
   * Keyboard accessible: tab through agent cards, open/close with keys.
   * High contrast toggle for accessibility.
8. **Agent Logs/Transparency**

   * Info/log icon on agent cards for: prompt sent, time taken, model used.
   * **Agent Rationale on Hover:**

     * Each agent (especially Critique and Debate) can optionally explain "why" it chose or improved an answer—shown on hover or in an info card. Example: “The Viz Agent picked bar chart because categorical breakdown was detected.”
9. **Report Download**

   * PDF download as floating glass button (bottom right when ready).

---

## 11. Detailed Repo Structure (Best Practice)

```
root/
├── frontend/                         # Next.js, D3.js, glassmorphic UI, assets
│   ├── components/                   # All React components (GlassCard, ChatBox, AgentPipeline, etc.)
│   ├── icons/                        # SVG/PNG icons for all agents (Lucide/custom)
│   ├── assets/                       # Logos, backgrounds, glass textures, D3.js viz templates
│   ├── pages/                        # Next.js routes (/, /api, /about, etc.)
│   ├── styles/                       # Tailwind, custom glassmorphic CSS, theme vars
│   ├── hooks/                        # React hooks (useAgentLog, useTheme, etc.)
│   ├── lib/                          # Frontend utility functions (data preview, validation)
│   ├── public/                       # Static assets, favicon, manifest
│   ├── tests/                        # Frontend Jest/unit tests
│   └── README.md                     # Frontend-specific docs
├── backend/                          # FastAPI, LangChain, LangGraph, agents
│   ├── main.py                       # API entrypoint
│   ├── agents/                       # Each agent logic as separate module (profile, planning, insight, viz, critique, debate, report)
│   ├── pipelines/                    # Agent orchestration, LangGraph pipeline code
│   ├── embeddings/                   # Pinecone client, chunking, embedding functions
│   ├── utils/                        # Helpers, error handling, validators
│   ├── logging/                      # Logging config, debug tools, trace formatting
│   ├── tests/                        # Backend pytest, mock LLM, integration tests
│   └── README.md                     # Backend-specific docs
├── docker/                           # Dockerfiles for frontend/backend/dev
├── .github/                          # GitHub Actions, workflows, issue templates
├── requirements.txt                  # Backend Python deps
├── package.json                      # Frontend JS deps
├── .env.example                      # Example environment variables
├── README.md                         # Root docs, architecture, usage
└── LICENSE
```

### Icon & Asset Strategy

* Store all icons in `frontend/icons/` (Lucide, shadcn/ui, custom SVGs). Each agent (Critique, Viz, Debate, etc.) should have a distinct icon—glassy/3D, accessible, scalable.
* Use `frontend/assets/` for logos, backgrounds, and D3.js visualization templates or themes.
* All assets/icons should support dark/light, theming, and a11y labels.

### Logging & Debugging Strategy

* **Frontend:**

  * Integrate Sentry or LogRocket for error and user replay.
  * Log all user/agent interactions, payloads, and errors to a state store (Redux/Zustand).
  * Add a dev toggle to show/hide verbose logs for each agent (ideal for QA/demo).
* **Backend:**

  * Use Python `logging` with separate log files:

    * `pipeline.log` for agent orchestration
    * `agent_{name}.log` for each agent (profiling, planning, etc.)
    * `error.log` for stack traces/exceptions
  * Integrate LangChain callback handlers and LangSmith for trace/debug.
  * All agent prompts, response times, and RAG retrievals are logged (with trace ID).
  * Optionally, return trace/logs with API response in dev mode.

### Project Creation/Build Strategy

1. **Plan:** Finalize architecture, data flow, wireframes. Agree on agent pipeline and UI/UX.
2. **Setup:** Init mono-repo, Docker, CI/CD. Create `.env`, baseline configs.
3. **Develop Agents:** Implement one agent at a time in backend/agents/. Test each step with mock data.
4. **Integrate Frontend:** Build 2-column UI, plug in chat, agent pipeline, file upload. Add glassmorphism, icons, D3.js.
5. **Connect RAG:** Wire up Pinecone and embedding pipeline, enable query-to-agent flow.
6. **Observability:** Add logging/debug infra, frontend dev mode, error reporting, LangSmith trace.
7. **Polish:** Add animated effects, onboarding, accessibility, real-time updates, PDF report.
8. **Test/Deploy:** End-to-end tests, deploy backend (Render), frontend (Vercel). Validate full flow.
9. **Document/Launch:** Fill out all READMEs, provide sample env, agent and UI diagrams, usage guide.

---

## 12. LangChain, LangGraph, and Advanced RAG Guidance

### LangChain

* Use LangChain as the core LLM orchestration library.
* Every agent is implemented as a LangChain Tool or Chain. Example: `@tool("data_profile_agent") def profile_data(...): ...`
* For insight/viz, use `ConversationalRetrievalChain` for context-aware queries.
* Use LangChain's PromptTemplates for modular, dynamic prompts—ensure each agent's logic is version-controlled.
* Integrate LangChain's callback handlers for pipeline transparency and observability (logging every step, time, prompt, and output for each agent).

### LangGraph

* Use LangGraph to define the entire agent pipeline as a **Directed Acyclic Graph (DAG)**—each agent is a node, transitions are workflow edges.
* Agents are chained as per your business logic: Data Profile → Planning → \[Insight/Viz] → Critique → Debate → Report.
* Enable conditional routing in LangGraph (e.g., Planning Agent decides branch: insight vs viz).
* Enable parallel or multi-agent debate nodes—allow multiple agents (different models or logic) to respond and route to Debate Agent for finalization.
* Support reactivity: on file change, trigger re-profiling and rerun dependent nodes.

### Advanced RAG (Retrieval Augmented Generation)

* At file upload, chunk data (rows, columns, or text blocks) and embed using the embedding model and configuration specified in `.env` (see OLLAMA\_MODEL, Pinecone config). No hardcoded or external model names are referenced in the implementation or documentation.
* Index all chunks in Pinecone (metadata = table name, column, data types, etc.).
* For each query, context assembly:

  * Retrieve top-k semantically similar chunks from Pinecone
  * (Optionally) retrieve by structured metadata (column, type, etc.)
  * Hybrid search: semantic + SQL schema awareness
  * Assemble retrievals into a custom system prompt for Insight or Viz Agent (LangChain RetrievalQA/ConversationalRetrievalChain)
* Viz Agent: retrieved context is processed, then `chart_type` is auto-selected using LLM function-calling (e.g., "bar if categorical, line if time series").
* Critique Agent also critiques retrieval context (“Too generic”, “Wrong date range”, etc.).
* Store full RAG trace per request for debugging and transparency.
* (Optional) Add self-improving retrieval—if agent result is critiqued as subpar, system retries with different context/parameters.

### Dev Best Practices

* Isolate agent logic per file/class/module for testability.
* Use LangChain Expression Language (LCEL) for custom chaining (v0.1+ recommended).
* Write test scripts (pytest, etc.) for each agent/tool/chain (mock LLM + mock data).
* Use LangSmith for tracing and debugging in development.

### Example: Agent Pipeline (LangGraph Code Skeleton)

```python
import langgraph
from agents import data_profile_agent, planning_agent, insight_agent, viz_agent, critique_agent, debate_agent, report_agent

with langgraph.Graph() as graph:
    upload = data_profile_agent()
    plan = planning_agent()
    insight = insight_agent()
    viz = viz_agent()
    critique = critique_agent()
    debate = debate_agent()
    report = report_agent()

    upload >> plan
    plan >> [insight, viz]
    insight >> critique
    viz >> critique
    critique >> debate
    debate >> report
```

---

## 17. Environment Configuration (.env Sample)

```
# ================================================================
# AGENTIC COPILOT - ENVIRONMENT CONFIGURATION
# ================================================================

# ================== PINECONE VECTOR DATABASE ===================
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=pineindex
PINECONE_HOST=https://pineindex-xxxxxxx.svc.aped-xxxx-yyyy.pinecone.io
PINECONE_DIMENSION=384
PINECONE_METRIC=cosine
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
PINECONE_TOP_K=10
PINECONE_INCLUDE_METADATA=true
PINECONE_INCLUDE_VALUES=false
PINECONE_BATCH_SIZE=100
PINECONE_MAX_RETRIES=3
PINECONE_TIMEOUT=30

# ================= OLLAMA CONFIGURATION (LLM) ===================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_TIMEOUT=120
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2048

# =========== LANGCHAIN & LANGSMITH CONFIGURATION ===============
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=agentic-copilot
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your_langsmith_api_key

# =============== FASTAPI APPLICATION CONFIGURATION =============
APP_NAME=Agentic Copilot
APP_VERSION=1.0.0
DEBUG=true
ENVIRONMENT=development
LOG_LEVEL=DEBUG
API_PREFIX=/api/v1

# ==================== SERVER CONFIGURATION =====================
HOST=127.0.0.1
PORT=8000
RELOAD=true
WORKERS=1

# ==================== CORS CONFIGURATION =======================
CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:3000","https://localhost:3000"]
CORS_CREDENTIALS=true
CORS_METHODS=["GET","POST","PUT","DELETE","OPTIONS","PATCH"]
CORS_HEADERS=["*"]

# ================= FILE UPLOAD CONFIGURATION ===================
UPLOAD_DIR=uploads
MAX_FILE_SIZE=100000000
ALLOWED_EXTENSIONS=["csv","xlsx","xls","json","txt","pdf"]
UPLOAD_TIMEOUT=300

# ================ DATABASE CONFIGURATION =======================
DATABASE_URL=sqlite:///./agentic_copilot.db
DATABASE_POOL_SIZE=5
DATABASE_POOL_TIMEOUT=30
DATABASE_ECHO=false

# =================== CACHING CONFIGURATION =====================
CACHE_TTL=300
CACHE_MAX_SIZE=1000
ENABLE_CACHING=true

# ================= SECURITY CONFIGURATION ======================
SECRET_KEY=your-secret-key-change-in-production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# =========== MONITORING & OBSERVABILITY ========================
ENABLE_METRICS=true
ENABLE_TRACING=true
DETAILED_LOGS=true
METRICS_PORT=9090

# =================== FEATURE FLAGS =============================
ENABLE_FILE_UPLOAD=true
ENABLE_DATA_PREVIEW=true
ENABLE_AI_AGENTS=true
ENABLE_RAG=true
ENABLE_ANALYTICS=true

# ============= EXTERNAL API CONFIGURATION ======================
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GOOGLE_AI_API_KEY=your_google_ai_api_key_here

# ================== DEVELOPMENT TOOLS ==========================
AUTO_RELOAD=true
SHOW_DOCS=true
INCLUDE_ADMIN_ROUTES=true

# ================ DEPLOYMENT CONFIGURATION =====================
RENDER_EXTERNAL_URL=https://your-app.onrender.com
VERCEL_URL=https://your-app.vercel.app

# =================== EMAIL CONFIGURATION =======================
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_password
EMAIL_FROM=your_email@gmail.com

# ============== BACKUP & STORAGE (Optional) ====================
BACKUP_ENABLED=false
BACKUP_SCHEDULE=0 2 * * *
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_S3_BUCKET=your-backup-bucket
AWS_REGION=us-east-1
```

# END OF AGENTIC COPILOT ARCHITECTURE CANVAS (HIERARCHICAL AGENT, GLASSMORPHIC UI)
